---
title: 二维目标检测总结
date: 2019-01-17 17:34:37
tags: [CV, 目标检测, 综述]
---

目标检测（Object Detection）是一项从图像中检测到物体的位置并区分物体类别的一项任务，即输出为检测框的四个坐标 ( x1,x2,y1,y2 ) 和物体的类别 y。目前有两种方式，一种为单阶段检测，以yolo和ssd为代表；另外一种为双阶段检测，以rcnn系列为代表。本文下面将总结这两种检测方法的思想和模型，以期望打好3D 物体检测的基础。

<!--more-->

# 单阶段

## yolo

> 论文下载：<http://arxiv.org/abs/1506.02640>
>
> 代码下载：<https://github.com/pjreddie/darknet> 

### 论文翻译

#### 摘要

> 方法的本质

过去的工作是使用**classfier**去进行检测（repurposes classifiers to perform detection），相反我们把目标检测设计为一种关于bounding boxes和class propabilities的**回归**问题，我们采用一个single神经网络只需要一次evaluation直接从图像中得到bounding boxes 和类别概率，因为整个检测流程是基于单个网络，所以可以直接根据detection performance 进行优化。

> 有三个优点：

1、我们的网络非常快，基本的model 可以达到每秒45frames，fast yolo可以达到155frame 每秒的同时保证mAP指标是其他方法的两倍。

2、与现在最好的方法比，YOLO在定位上（localization）错误率更高但是在背景上预测的错误率更小。

3、最后，YOLO的对物体的泛化表示更好，相比较于RCNN和DPM，在从自然图像到艺术作品的泛化性能更好。

#### 1. 介绍

> 快速识别物体的重要性

人类看了一眼图像，立刻就知道图像中的对象是什么，它们在哪里，以及它们是如何相互作用的。人类的视觉系统是快速和准确的，允许我们执行复杂的任务，如驾驶时，很少有清醒的思考过程。快速、准确的物体检测算法将使计算机能够在没有特殊传感器的情况下驾驶汽车，使辅助设备能够向人类用户传递实时场景信息，并为通用、响应性的机器人系统挖掘潜力。

> 当前主流方法的简要介绍

当前的检测系统将分类器重新使用分类器来检测。为了检测对象，这些系统为该对象提取一个分类器，并在测试图像中的不同位置和尺度上对其进行评估。

1、像DPM模型这样的系统使用滑动窗口方法，其中分类器在整个图像上的均匀间隔位置运行；

2、而RCNN模型是使用RPN办法在图像上首先生成潜在的bbox，然后再在这些盒子上使用分类器，之后再用一些后期处理重新定位bbox,消除重复的检测，并根据场景中的其他对象重新计算得分。

3、这些复杂的方法因为是单独地被训练的，所以很难被优化。

> 本文方法的简介

我们把目标检测重新设计为了一种单一的回归问题，直接从图像的pixels到bbox的坐标和类别概率。一个CNN网络同时预测了多个bbox的坐标和类别概率。YOLO在整个图片上训练并且直接优化detection的性能，下面是与传统方法的benifits:

![屏幕快照 2019-01-17 下午9.58.09](https://ws4.sinaimg.cn/large/006tNc79ly1fz9xewejfej30j809udjn.jpg)

First，YOLO速度非常快。由于我们设计的检测系统实质上是一个回归问题，我们不需要一个复杂的pipeline。在测试时我们只需在新图像上运行我们的神经网络就能预测检测结果。我们的基本网络在没有批处理的Titan X GPU运行45帧每秒，在一个快速版本运行超过150 fps。这意味着我们可以在少于25毫秒的延迟时间内实时处理流视频。此外，YOLO的平均精度是其他实时系统的两倍以上。

Second，YOLO在做预测时能够全局思考图像。与基于滑动窗口sliding window和区域提案region proposal的技术不同，YOLO在训练和测试期间看到了整个图像，因此它隐式编码了有关类的上下文信息以及它们的外观。Fast R-CNN会把背景上的一小块看作是一个object，因为它不能看到范围更广的信息，而YOLO比起Fast R-CNN背景的错误率降低了一半。

Third，YOLO学习对象的泛化表示。在对自然图像进行训练并在artwork上进行测试时，YOLO远远超过了DPM和R-CNN这样的顶级检测方法。由于YOLO具有很高的泛化能力，所以当应用于新的场景或有unexpected inputs时，不太可能发生问题。

最后，YOLO在准确性方面仍然落后于最先进的检测系统。虽然它能快速识别出图像中的物体，但它很难精确地定位一些物体，尤其是小型物体。

#### 2. 统一检测

我们将对象检测的独立组件统一到单个神经网络中。我们的网络使用来自整个图像的特征来预测每个边界框。它还可以同时预测所有类别中和所有边界框。这就意味着我们的网络原因是围绕整个图像和图像中的所有对象。Yolo设计能够实现端到端培训和实时速度，同时保持较高的平均精度。

![屏幕快照 2019-01-18 下午4.27.02](https://ws3.sinaimg.cn/large/006tNc79ly1fzatgwbym3j30hm0gen53.jpg)

> 具体方法细节

1、将输入图像划分为S*S 个grid。如果center of object落入一个grid cell中，则该cell负责检测该object。（**每个cell预测B个bbox和这些框的置信度分数**）

- 正式地定义置信度得分：Pr(Object)✖️IOU
- 如果没有物体存在这个cell里面，Pr(Object)为0，也就是置信度为0，
- 否则的话，Pr(object)为1，置信度得分就为IOU的值。

2、每个bbox包括 5个预测值： x, y, w, h, confidence。

- (x，y)坐标表示盒的中心相对于网格单元格边界的距离；

- 宽度和高度是相对于整个图像进行预测的；

- 置信度预测表示预测的盒子与ground_truth之间的IOU。

3、每个cell还预测C个类别条件概率Pr(Class|Object)，这些概率取决于包含对象的网格单元格，我们只预测每个网格单元格的一组类概率，而不管bbox框的数量。

在测试时，我们将条件类概率和单个盒置信度预测相乘，就得到每个框预测得到的特定类别的置信度分数，这些分数既编码了该类出现在框中的概率，也表示了预测的框和对象的适应程度。![屏幕快照 2019-01-18 下午4.22.35](https://ws2.sinaimg.cn/large/006tNc79ly1fzatbyusckj30gg020q2z.jpg)

##### 2.1. 网络设计

我们将该模型实现为一个卷积神经网络，并在Pascal VOC检测数据集上进行了evaluate。网络的初始卷积层从图像中提取特征，全连通层预测输出概率和坐标。

我们的网络架构是受Google网图像分类模型的启发。我们的网络有24个卷积层接着2个全连接层。与谷歌使用的Inception模块不同，我们只使用了1✖️1reduction层和3✖️3卷积层。完整的网络如图3所示![屏幕快照 2019-01-18 下午4.39.14](https://ws1.sinaimg.cn/large/006tNc79ly1fzatt90wufj311a0g0mzr.jpg)

##### 2.2. Training

我们在imagnet数据集上预训练，预训练时我们使用了图3中的前20层接着一个average-pooling layer 和一个 fully connected layer，我们训练了这个网络一周，实现了在2012年ImageNet验证集上面top5的88%的准确率，